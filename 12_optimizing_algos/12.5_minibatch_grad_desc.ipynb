{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78689128",
   "metadata": {},
   "source": [
    "# 12.5. Minibatch Stochastic Gradient Descent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1a37a9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/athapar/miniconda3/lib/python3.11/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "\n",
    "A = torch.zeros(256, 256)\n",
    "B = torch.randn(256, 256)\n",
    "C = torch.randn(256, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5f66c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Timer:  #@save\n",
    "    \"\"\"Record multiple running times.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.times = []\n",
    "        self.start()\n",
    "\n",
    "    def start(self):\n",
    "        \"\"\"Start the timer.\"\"\"\n",
    "        self.tik = time.time()\n",
    "\n",
    "    def stop(self):\n",
    "        \"\"\"Stop the timer and record the time in a list.\"\"\"\n",
    "        self.times.append(time.time() - self.tik)\n",
    "        return self.times[-1]\n",
    "\n",
    "    def avg(self):\n",
    "        \"\"\"Return the average time.\"\"\"\n",
    "        return sum(self.times) / len(self.times)\n",
    "\n",
    "    def sum(self):\n",
    "        \"\"\"Return the sum of time.\"\"\"\n",
    "        return sum(self.times)\n",
    "\n",
    "    def cumsum(self):\n",
    "        \"\"\"Return the accumulated time.\"\"\"\n",
    "        return np.array(self.times).cumsum().tolist()\n",
    "\n",
    "timer = Timer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3879e7da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6120688915252686"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute A = BC one element at a time\n",
    "timer.start()\n",
    "for i in range(256):\n",
    "    for j in range(256):\n",
    "        A[i, j] = torch.dot(B[i, :], C[:, j])\n",
    "timer.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec824b0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.014968633651733398"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute A = BC one column at a time\n",
    "timer.start()\n",
    "for j in range(256):\n",
    "    A[:, j] = torch.mv(B, C[:, j])\n",
    "timer.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc2392f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "performance in Gigaflops: element 0.049, column 2.004, full 4.622\n"
     ]
    }
   ],
   "source": [
    "# Compute A = BC in one go\n",
    "timer.start()\n",
    "A = torch.mm(B, C)\n",
    "timer.stop()\n",
    "\n",
    "gigaflops = [0.03 / i for i in timer.times]\n",
    "print(f'performance in Gigaflops: element {gigaflops[0]:.3f}, '\n",
    "      f'column {gigaflops[1]:.3f}, full {gigaflops[2]:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ddb8f1b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "performance in Gigaflops: block 9.568\n"
     ]
    }
   ],
   "source": [
    "timer.start()\n",
    "for j in range(0, 256, 64):\n",
    "    A[:, j:j+64] = torch.mm(B, C[:, j:j+64])\n",
    "timer.stop()\n",
    "print(f'performance in Gigaflops: block {0.03 / timer.times[-1]:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18f950cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "d2l.DATA_HUB['airfoil'] = (d2l.DATA_URL + 'airfoil_self_noise.dat',\n",
    "                           '76e5be1548fd8222e5074cf0faae75edff8cf93f')\n",
    "\n",
    "#@save\n",
    "def get_data_ch11(batch_size=10, n=1500):\n",
    "    data = np.genfromtxt(d2l.download('airfoil'),\n",
    "                         dtype=np.float32, delimiter='\\t')\n",
    "    data = torch.from_numpy((data - data.mean(axis=0)) / data.std(axis=0))\n",
    "    data_iter = d2l.load_array((data[:n, :-1], data[:n, -1]),\n",
    "                               batch_size, is_train=True)\n",
    "    return data_iter, data.shape[1]-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c497b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(params, states, hyperparams):\n",
    "    for p in params:\n",
    "        p.data.sub_(hyperparams['lr'] * p.grad)\n",
    "        p.grad.data.zero_()\n",
    "        \n",
    "#@save\n",
    "def train_ch11(trainer_fn, states, hyperparams, data_iter,\n",
    "               feature_dim, num_epochs=2):\n",
    "    # Initialization\n",
    "    w = torch.normal(mean=0.0, std=0.01, size=(feature_dim, 1),\n",
    "                     requires_grad=True)\n",
    "    b = torch.zeros((1), requires_grad=True)\n",
    "    net, loss = lambda X: d2l.linreg(X, w, b), d2l.squared_loss\n",
    "    # Train\n",
    "    animator = d2l.Animator(xlabel='epoch', ylabel='loss',\n",
    "                            xlim=[0, num_epochs], ylim=[0.22, 0.35])\n",
    "    n, timer = 0, d2l.Timer()\n",
    "    for _ in range(num_epochs):\n",
    "        for X, y in data_iter:\n",
    "            l = loss(net(X), y).mean()\n",
    "            l.backward()\n",
    "            trainer_fn([w, b], states, hyperparams)\n",
    "            n += X.shape[0]\n",
    "            if n % 200 == 0:\n",
    "                timer.stop()\n",
    "                animator.add(n/X.shape[0]/len(data_iter),\n",
    "                             (d2l.evaluate_loss(net, data_iter, loss),))\n",
    "                timer.start()\n",
    "    print(f'loss: {animator.Y[0][-1]:.3f}, {timer.sum()/num_epochs:.3f} sec/epoch')\n",
    "    return timer.cumsum(), animator.Y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb96ac13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.251, 0.010 sec/epoch\n"
     ]
    }
   ],
   "source": [
    "def train_sgd(lr, batch_size, num_epochs=2):\n",
    "    data_iter, feature_dim = get_data_ch11(batch_size)\n",
    "    return train_ch11(\n",
    "        sgd, None, {'lr': lr}, data_iter, feature_dim, num_epochs)\n",
    "\n",
    "gd_res = train_sgd(1, 1500, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1d32746a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.244, 0.177 sec/epoch\n"
     ]
    }
   ],
   "source": [
    "sgd_res = train_sgd(0.005, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4dc71c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.243, 0.017 sec/epoch\n"
     ]
    }
   ],
   "source": [
    "mini1_sgd = train_sgd(.4, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3c70f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.250, 0.028 sec/epoch\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mini2_sgd = train_sgd(.05, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e74ddda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "d2l.set_figsize([6, 3])\n",
    "d2l.plot(*list(map(list, zip(gd_res,sgd_res , mini1_sgd, mini2_sgd))),\n",
    "         'time (sec)', 'loss', xlim=[1e-2, 10],\n",
    "         legend=['gd', 'sgd', 'batch size=100', 'batch size=10'])\n",
    "d2l.plt.gca().set_xscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c458b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "def train_concise_ch11(trainer_fn, hyperparams, data_iter, num_epochs=4):\n",
    "    # Initialization\n",
    "    net = nn.Sequential(nn.Linear(5, 1))\n",
    "    def init_weights(module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, std=0.01)\n",
    "    net.apply(init_weights)\n",
    "\n",
    "    optimizer = trainer_fn(net.parameters(), **hyperparams)\n",
    "    loss = nn.MSELoss(reduction='none')\n",
    "    animator = d2l.Animator(xlabel='epoch', ylabel='loss',\n",
    "                            xlim=[0, num_epochs], ylim=[0.22, 0.35])\n",
    "    n, timer = 0, d2l.Timer()\n",
    "    for _ in range(num_epochs):\n",
    "        for X, y in data_iter:\n",
    "            optimizer.zero_grad()\n",
    "            out = net(X)\n",
    "            y = y.reshape(out.shape)\n",
    "            l = loss(out, y)\n",
    "            l.mean().backward()\n",
    "            optimizer.step()\n",
    "            n += X.shape[0]\n",
    "            if n % 200 == 0:\n",
    "                timer.stop()\n",
    "                # `MSELoss` computes squared error without the 1/2 factor\n",
    "                animator.add(n/X.shape[0]/len(data_iter),\n",
    "                             (d2l.evaluate_loss(net, data_iter, loss) / 2,))\n",
    "                timer.start()\n",
    "    print(f'loss: {animator.Y[0][-1]:.3f}, {timer.sum()/num_epochs:.3f} sec/epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ea577d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.242, 0.042 sec/epoch\n"
     ]
    }
   ],
   "source": [
    "data_iter, _ = get_data_ch11(10)\n",
    "trainer = torch.optim.SGD\n",
    "train_concise_ch11(trainer, {'lr': 0.01}, data_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1910c7f5",
   "metadata": {},
   "source": [
    "# tower of hanoi\n",
    "def hanoi(n, from_rod, to_rod, aux_rod):\n",
    "    if n == 0:\n",
    "        return\n",
    "    hanoi(n - 1, from_rod, aux_rod, to_rod)\n",
    "    print(f\"move disk {n} from rod {from_rod} to rod {to_rod}\")\n",
    "    hanoi(n - 1, aux_rod, to_rod, from_rod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe9fd4b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "move disk 1 from rod A to rod C\n",
      "move disk 2 from rod A to rod B\n",
      "move disk 1 from rod C to rod B\n",
      "move disk 3 from rod A to rod C\n",
      "move disk 1 from rod B to rod A\n",
      "move disk 2 from rod B to rod C\n",
      "move disk 1 from rod A to rod C\n"
     ]
    }
   ],
   "source": [
    "hanoi(3, \"A\", \"C\", \"B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc60666f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9e29066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.grad: tensor([ 2.,  4., 68., 10.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1, 2, 34, 5], dtype=torch.float32, requires_grad=True)\n",
    "y = x **2\n",
    "y.sum().backward()\n",
    "print(f\"x.grad: {x.grad}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
