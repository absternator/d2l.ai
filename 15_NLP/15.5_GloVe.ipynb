{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9560b55",
   "metadata": {},
   "source": [
    "# 15.5. Word Embedding with Global Vectors (GloVe)\n",
    "\n",
    "Global Vectors for Word Representation (GloVe) is an unsupervised learning algorithm for obtaining vector representations of words. GloVe combines the advantages of two major model families in the literature: global matrix factorization and local context window methods.\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "**Global Matrix Factorization**: Methods like Latent Semantic Analysis (LSA) efficiently leverage statistical information by training on global word-word co-occurrence counts, but perform poorly on word analogy tasks.\n",
    "\n",
    "**Local Context Window Methods**: Methods like skip-gram model may perform better on analogy tasks but poorly utilize the statistics of the corpus since they train on separate local context windows.\n",
    "\n",
    "**GloVe Model**: Combines both approaches by training on global word-word co-occurrence statistics in a way that can produce meaningful linear substructures in the word vector space.\n",
    "\n",
    "## How GloVe Works\n",
    "\n",
    "1. **Co-occurrence Matrix**: Construct a word-word co-occurrence matrix from the corpus\n",
    "2. **Objective Function**: Minimize a weighted least squares regression model that relates word vectors to global co-occurrence counts\n",
    "3. **Training**: Learn word vectors that capture meaningful relationships between words\n",
    "\n",
    "## Advantages\n",
    "\n",
    "- Captures global corpus statistics effectively\n",
    "- Produces meaningful vector space structures\n",
    "- Computationally efficient\n",
    "- Good performance on word analogy and similarity tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b9296722",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "symbols = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm',\n",
    "           'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z',\n",
    "           '_', '[UNK]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df61d2b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f a s t _': 4, 'f a s t e r _': 3, 't a l l _': 5, 't a l l e r _': 4}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_token_freqs = {'fast_': 4, 'faster_': 3, 'tall_': 5, 'taller_': 4}\n",
    "token_freqs = {}\n",
    "for token, freq in raw_token_freqs.items():\n",
    "    token_freqs[\" \".join(list(token))] = freq\n",
    "\n",
    "token_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8208eff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_freq_pair(token_freqs):\n",
    "    pairs = collections.defaultdict(int)\n",
    "    for token, freq in token_freqs.items():\n",
    "        symbols = token.split()\n",
    "        for i in range(len(symbols) - 1):\n",
    "            # Key of `pairs` is a tuple of two consecutive symbols\n",
    "            pairs[symbols[i], symbols[i + 1]] += freq\n",
    "    \n",
    "    return max(pairs, key=pairs.get)  # Key of `pairs` with the max value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b75ee1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_symbols(max_freq_pair, token_freqs, symbols):\n",
    "    symbols.append(''.join(max_freq_pair))\n",
    "    new_token_freqs = dict()\n",
    "    for token, freq in token_freqs.items():\n",
    "        new_token = token.replace(' '.join(max_freq_pair),\n",
    "                                  ''.join(max_freq_pair))\n",
    "        new_token_freqs[new_token] = token_freqs[token]\n",
    "    return new_token_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e746eda7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merge #1: ('t', 'a')\n",
      "merge #2: ('ta', 'l')\n",
      "merge #3: ('tal', 'l')\n",
      "merge #4: ('f', 'a')\n",
      "merge #5: ('fa', 's')\n",
      "merge #6: ('fas', 't')\n",
      "merge #7: ('e', 'r')\n",
      "merge #8: ('er', '_')\n",
      "merge #9: ('tall', '_')\n",
      "merge #10: ('fast', '_')\n"
     ]
    }
   ],
   "source": [
    "num_merges = 10\n",
    "for i in range(num_merges):\n",
    "    max_freq_pair = get_max_freq_pair(token_freqs)\n",
    "    token_freqs = merge_symbols(max_freq_pair, token_freqs, symbols)\n",
    "    print(f'merge #{i + 1}:', max_freq_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a148af21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '_', '[UNK]', 'ta', 'tal', 'tall', 'fa', 'fas', 'fast', 'er', 'er_', 'tall_', 'fast_']\n",
      "dict_keys(['fast_', 'fast er_', 'tall_', 'tall er_'])\n"
     ]
    }
   ],
   "source": [
    "print(symbols)\n",
    "print(token_freqs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b1288182",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_BPE(tokens, symbols):\n",
    "    outputs = []\n",
    "    for token in tokens:\n",
    "        start, end = 0, len(token)\n",
    "        cur_output = []\n",
    "        # Segment token with the longest possible subwords from symbols\n",
    "        while start < len(token) and start < end:\n",
    "            if token[start: end] in symbols:\n",
    "                cur_output.append(token[start: end])\n",
    "                start = end\n",
    "                end = len(token)\n",
    "            else:\n",
    "                end -= 1\n",
    "        if start < len(token):\n",
    "            cur_output.append('[UNK]')\n",
    "        outputs.append(' '.join(cur_output))\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4ab62749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tall e s t _', 'fa t t er_']\n"
     ]
    }
   ],
   "source": [
    "tokens = ['tallest_', 'fatter_']\n",
    "print(segment_BPE(tokens, symbols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99832e41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
