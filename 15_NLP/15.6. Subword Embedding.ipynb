{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "987f9d22",
   "metadata": {},
   "source": [
    "# 15.6. Subword Embedding\n",
    "\n",
    "Subword embedding is a technique that addresses the limitations of word-level embeddings by breaking words into smaller, meaningful units. This approach helps handle out-of-vocabulary (OOV) words and morphologically rich languages more effectively.\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "- **Subword units**: Smaller components of words such as characters, character n-grams, or learned subword segments\n",
    "- **OOV handling**: Ability to represent previously unseen words by combining known subword components\n",
    "- **Morphological awareness**: Better representation of word formations and linguistic patterns\n",
    "\n",
    "## Common Approaches\n",
    "\n",
    "1. **Character-level embeddings**: Treat each character as a basic unit\n",
    "2. **Byte Pair Encoding (BPE)**: Iteratively merge frequent character pairs\n",
    "3. **WordPiece**: Google's tokenization method used in BERT\n",
    "4. **SentencePiece**: Language-agnostic subword tokenization\n",
    "\n",
    "## Advantages\n",
    "\n",
    "- Reduces vocabulary size while maintaining semantic information\n",
    "- Handles rare and compound words effectively\n",
    "- Improves performance on morphologically complex languages\n",
    "- Enables better generalization to unseen words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9192d95",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
